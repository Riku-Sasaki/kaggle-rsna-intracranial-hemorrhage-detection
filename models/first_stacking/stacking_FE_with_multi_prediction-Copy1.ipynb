{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = ['appian', 'sasaki_se_resnext101_mixup_410_with_imagenet_norm', 'tsukiyama_inceotionv4', 'seresnext410', 'sasaki_senet154_customlabels', 'tsukiyama_xception_224', 'sugawara_efficientnetb3', 'tsukiyama_inception_resnet_v2_336']\n",
    "model_list = ['tsukiyama_inceotionv4', 'seresnext410']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('max_rows', 100)\n",
    "def wlogloss(targets, preds):\n",
    "    target_cols = ['any', 'epidural', 'subdural', 'subarachnoid', 'intraventricular', 'intraparenchymal']\n",
    "    res = 0\n",
    "    for col in target_cols:\n",
    "        res += log_loss(targets[col], preds[col+'_pred'])\n",
    "        if col == 'any':\n",
    "            res += log_loss(targets[col], preds[col+'_pred'])\n",
    "    res /= 7\n",
    "    return res\n",
    "tr_meta = pd.read_pickle('../cache/train_raw.pkl')\n",
    "ts_meta = pd.read_pickle('../cache/test_raw.pkl')\n",
    "\n",
    "target_cols = ['any', 'epidural', 'subdural', 'subarachnoid', 'intraventricular', 'intraparenchymal']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_tta = 5\n",
    "\n",
    "def get_train_df(path):\n",
    "    # train data\n",
    "    df_all = []\n",
    "    for n_fold in range(5):\n",
    "        df = pd.read_pickle(f'{path}/fold{n_fold}_ep2_valid_tta5.pkl')\n",
    "        # 予測値\n",
    "        tmp = np.zeros([len(df[0]['ids']), 6])\n",
    "        for i in range(n_tta):\n",
    "            tmp += df[i]['outputs'] / n_tta\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp.columns = [tar_col + '_pred' for tar_col in target_cols]\n",
    "        tmp['ID'] = df[0]['ids']\n",
    "        tmp['folds'] = n_fold\n",
    "        # 実測値\n",
    "        tmp2 = pd.DataFrame(df[0]['targets'], columns = target_cols)\n",
    "        df_all.append(pd.concat([tmp, tmp2], axis=1))\n",
    "    \n",
    "    df_all = pd.concat(df_all)\n",
    "    train = pd.merge(df_all, tr_meta, on='ID', how='inner')\n",
    "    return train\n",
    "\n",
    "def get_test_df_list(path):\n",
    "    # test data\n",
    "    df_all_ts = []\n",
    "    for n_fold in range(5):\n",
    "        df = pd.read_pickle(f'{path}/fold{n_fold}_ep2_test_tta5.pkl')\n",
    "        tmp = np.zeros([len(df[0]['ids']), 6])\n",
    "        for i in range(n_tta):\n",
    "            tmp += df[i]['outputs'] / n_tta\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp.columns = [tar_col + '_pred' for tar_col in target_cols]\n",
    "        tmp['ID'] = df[n_fold]['ids']\n",
    "        tmp['folds'] = n_fold\n",
    "        tmp = pd.merge(tmp, ts_meta, on='ID', how='inner')\n",
    "        df_all_ts.append(tmp)\n",
    "    return df_all_ts\n",
    "\n",
    "\n",
    "def ortho_df(df):\n",
    "    df['ImagePositionPatient_2'] = df['ImagePositionPatient'].progress_apply(lambda x: x[2])\n",
    "    df = df.merge(df.groupby(\n",
    "        ['StudyInstanceUID']\n",
    "    )['ImagePositionPatient_2'].agg(position_min='min', position_max='max').reset_index(), on='StudyInstanceUID')\n",
    "    df['position'] = (df['ImagePositionPatient_2'] - df['position_min']) / (df['position_max'] - df['position_min'])\n",
    "    res = df.sort_values(by=['StudyInstanceUID', 'position'])\n",
    "    return res\n",
    "\n",
    "def pred_agg1(df):\n",
    "    new_feats = []\n",
    "    for c in target_cols:\n",
    "        tmp = df.groupby(\n",
    "            ['StudyInstanceUID']\n",
    "        )[c+'_pred'].agg(['min', 'max', 'mean', 'std']).reset_index()\n",
    "        tmp.columns = ['StudyInstanceUID', c+'_min', c+'_max', c+'_mean', c+'_std']\n",
    "        if c != 'any':\n",
    "            del tmp['StudyInstanceUID']\n",
    "        new_feats.append(tmp)\n",
    "    new_feats = pd.concat(new_feats, axis=1)\n",
    "    df = pd.merge(df, new_feats, on='StudyInstanceUID', how='left')\n",
    "\n",
    "    new_feats = []\n",
    "    for c in target_cols:\n",
    "        tmp = df.groupby(\n",
    "            ['PatientID']\n",
    "        )[c+'_pred'].agg(['min', 'max', 'mean', 'std']).reset_index()\n",
    "        tmp.columns = ['PatientID', c+'_min_PatientID', c+'_max_PatientID', c+'_mean_PatientID', c+'_std_PatientID']\n",
    "        if c != 'any':\n",
    "            del tmp['PatientID']\n",
    "        new_feats.append(tmp)\n",
    "    new_feats = pd.concat(new_feats, axis=1)\n",
    "    df = pd.merge(df, new_feats, on='PatientID', how='left')\n",
    "    \n",
    "    for c in target_cols:\n",
    "        df[c+'_diff'] = df[c+'_pred'] - df[c+'_mean']\n",
    "        df[c+'_div'] = df[c+'_pred'] / df[c+'_mean']\n",
    "        df[c+'_scaled'] = (df[c+'_pred'] - df[c+'_mean']) / df[c+'_std']\n",
    "    return df\n",
    "\n",
    "def pred_agg2(df):\n",
    "    a1 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(3, min_periods=1, center=True).mean().values\n",
    "    a2 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(5, min_periods=1, center=True).mean().values\n",
    "    new_feats1 = pd.DataFrame(a1, columns = [c+'_3roll' for c in target_cols])\n",
    "    new_feats2 = pd.DataFrame(a2, columns = [c+'_5roll' for c in target_cols])\n",
    "    new_feats1.index = df.index\n",
    "    new_feats2.index = df.index\n",
    "    a3 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(1, min_periods=1, center=True).mean().values\n",
    "    new_feats17 = pd.DataFrame(a1 / a3, columns = [c+'_3rolldiv' for c in target_cols])\n",
    "    new_feats18 = pd.DataFrame(a2 / a3, columns = [c+'_5rolldiv' for c in target_cols])\n",
    "    new_feats17.index = df.index\n",
    "    new_feats18.index = df.index\n",
    "\n",
    "    a3 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(7, min_periods=1, center=True).mean().values\n",
    "    a4 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(9, min_periods=1, center=True).mean().values\n",
    "    new_feats3 = pd.DataFrame(a3, columns = [c+'_7roll' for c in target_cols])\n",
    "    new_feats4 = pd.DataFrame(a4, columns = [c+'_9roll' for c in target_cols])\n",
    "    new_feats3.index = df.index\n",
    "    new_feats4.index = df.index\n",
    "    \n",
    "    a5 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(3, min_periods=1, center=True).min().values\n",
    "    a6 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(5, min_periods=1, center=True).min().values\n",
    "    new_feats5 = pd.DataFrame(a5, columns = [c+'_3rollmin' for c in target_cols])\n",
    "    new_feats6 = pd.DataFrame(a6, columns = [c+'_5rollmin' for c in target_cols])\n",
    "    new_feats5.index = df.index\n",
    "    new_feats6.index = df.index\n",
    "    a7 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(7, min_periods=1, center=True).min().values\n",
    "    a8 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(9, min_periods=1, center=True).min().values\n",
    "    new_feats7 = pd.DataFrame(a3, columns = [c+'_7rollmin' for c in target_cols])\n",
    "    new_feats8 = pd.DataFrame(a4, columns = [c+'_9rollmin' for c in target_cols])\n",
    "    new_feats7.index = df.index\n",
    "    new_feats8.index = df.index\n",
    "\n",
    "    a9 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(3, min_periods=1, center=True).max().values\n",
    "    a10 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(5, min_periods=1, center=True).max().values\n",
    "    new_feats9 = pd.DataFrame(a9, columns = [c+'_3rollmax' for c in target_cols])\n",
    "    new_feats10 = pd.DataFrame(a10, columns = [c+'_5rollmax' for c in target_cols])\n",
    "    new_feats9.index = df.index\n",
    "    new_feats10.index = df.index\n",
    "    a11 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(7, min_periods=1, center=True).max().values\n",
    "    a12 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(9, min_periods=1, center=True).max().values\n",
    "    new_feats11 = pd.DataFrame(a11, columns = [c+'_7rollmax' for c in target_cols])\n",
    "    new_feats12 = pd.DataFrame(a12, columns = [c+'_9rollmax' for c in target_cols])\n",
    "    new_feats11.index = df.index\n",
    "    new_feats12.index = df.index\n",
    "\n",
    "    a13 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(3, min_periods=1, center=True).max().values\n",
    "    a14 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(5, min_periods=1, center=True).max().values\n",
    "    new_feats13 = pd.DataFrame(a9, columns = [c+'_3rollstd' for c in target_cols])\n",
    "    new_feats14 = pd.DataFrame(a10, columns = [c+'_5rollstd' for c in target_cols])\n",
    "    new_feats13.index = df.index\n",
    "    new_feats14.index = df.index\n",
    "    a15 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(7, min_periods=1, center=True).std().values\n",
    "    a16 = df.groupby('StudyInstanceUID')[[col for col in df.columns if col.endswith('_pred')]].rolling(9, min_periods=1, center=True).std().values\n",
    "    new_feats15 = pd.DataFrame(a15, columns = [c+'_7rollstd' for c in target_cols])\n",
    "    new_feats16 = pd.DataFrame(a16, columns = [c+'_9rollstd' for c in target_cols])\n",
    "    new_feats15.index = df.index\n",
    "    new_feats16.index = df.index\n",
    "    df = pd.concat([df, new_feats1, new_feats2, new_feats3, new_feats4, new_feats5, new_feats6\n",
    "                   , new_feats7, new_feats8, new_feats9, new_feats10, new_feats11, new_feats12, new_feats13, new_feats14, new_feats15, new_feats16, new_feats17, new_feats18], axis=1)\n",
    "#     df = pd.concat([df, new_feats1, new_feats2, new_feats17, new_feats18], axis=1)\n",
    "    return df\n",
    "\n",
    "def make_feats(df, model_name='appian', df_type = 'train'):\n",
    "    df = ortho_df(df)\n",
    "    df = pred_agg1(df)\n",
    "    df = pred_agg2(df)\n",
    "    target_str = '{}|{}|{}|{}|{}|{}'.format(target_cols[0], target_cols[1], target_cols[2], target_cols[3], target_cols[4], target_cols[5])\n",
    "    if df_type == 'train':\n",
    "        X_cols = [c for c in df.columns.drop(target_cols) if len(re.findall(target_str, c)) > 0]\n",
    "        df = df[['ID', 'folds', 'StudyInstanceUID'] + X_cols + target_cols]\n",
    "        df.columns = ['ID', 'folds', 'StudyInstanceUID'] + [model_name + '_' + c for c in X_cols] + target_cols\n",
    "    elif df_type =='test':\n",
    "        X_cols = [c for c in df.columns if len(re.findall(target_str, c)) > 0]\n",
    "        df = df[['ID', 'folds', 'StudyInstanceUID'] + X_cols]\n",
    "        df.columns = ['ID', 'folds', 'StudyInstanceUID'] + [model_name + '_' + c for c in X_cols]        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list = []\n",
    "for model in model_list:\n",
    "    path = f'../prediction/{model}/'\n",
    "    train = get_train_df(path)\n",
    "    train = make_feats(train, model, 'train')\n",
    "    X_cols = train.columns.drop(['ID', 'folds', 'StudyInstanceUID'] + target_cols).tolist()\n",
    "    train_df_list.append(train[['ID'] + X_cols])\n",
    "train_df_list.append(train[train.columns.drop(X_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df_list[0]\n",
    "for tmp_train in train_df_list[1:]:\n",
    "    train = pd.merge(train, tmp_train, on='ID', how='outer')\n",
    "X_cols = train.columns.drop(['ID', 'folds', 'StudyInstanceUID'] + target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "path = f'../prediction/appian/'\n",
    "test_list = get_test_df_list(path)\n",
    "test_model_list = []\n",
    "# モデルごとのループ\n",
    "for model in model_list:\n",
    "    path = f'../prediction/{model}/'\n",
    "    test_pred_original_list = get_test_df_list(path)\n",
    "    test_list = []\n",
    "    \n",
    "    # foldごとの予測値を取り出すループ\n",
    "    for test_tmp in test_pred_original_list:\n",
    "        test = make_feats(test_tmp, model, 'test')\n",
    "        X_cols = test.columns.drop(['ID', 'folds', 'StudyInstanceUID']).tolist()\n",
    "        test_list.append(test[['ID'] + X_cols])\n",
    "    test_model_list.append(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "# fold毎に各モデルの予測値が横につながった1csvになるようにする\n",
    "for n_fold in range(5):\n",
    "    tmp = []\n",
    "    for n_model in range(len(model_list)-1):\n",
    "        tmp.append(test_model_list[n_model][n_fold])\n",
    "    test = tmp[0]\n",
    "    for tmp_test in tmp[1:]:\n",
    "        test = pd.merge(test, tmp_test, on='ID', how='outer')\n",
    "    test_list.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_stackfeats_3_4.pickle', 'wb') as f:\n",
    "    pickle.dump(test_list, f)\n",
    "with open('train_stackfeats_3_4.pickle', 'wb') as f:\n",
    "    pickle.dump(train, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_pickle('test_stackfeats_3_4.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
