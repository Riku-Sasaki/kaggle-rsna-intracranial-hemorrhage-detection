{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('max_rows', 100)\n",
    "def wlogloss(targets, preds):\n",
    "    target_cols = ['any', 'epidural', 'subdural', 'subarachnoid', 'intraventricular', 'intraparenchymal']\n",
    "    res = 0\n",
    "    for col in target_cols:\n",
    "        res += log_loss(targets[col], preds[col+'_pred'])\n",
    "        if col == 'any':\n",
    "            res += log_loss(targets[col], preds[col+'_pred'])\n",
    "    res /= 7\n",
    "    return res\n",
    "tr_meta = pd.read_pickle('../cache/train_raw.pkl')\n",
    "ts_meta = pd.read_pickle('../cache/test_raw.pkl')\n",
    "\n",
    "target_cols = ['any', 'epidural', 'subdural', 'subarachnoid', 'intraventricular', 'intraparenchymal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_stackfeats_1_2.pickle', 'rb') as f:\n",
    "    test_list1 = pickle.load(f)\n",
    "with open('train_stackfeats_1_2.pickle', 'rb') as f:\n",
    "    train1 = pickle.load(f)\n",
    "with open('test_stackfeats_3_4.pickle', 'rb') as f:\n",
    "    test_list2 = pickle.load(f)\n",
    "with open('train_stackfeats_3_4.pickle', 'rb') as f:\n",
    "    train2 = pickle.load(f)\n",
    "with open('test_stackfeats_5_6.pickle', 'rb') as f:\n",
    "    test_list3 = pickle.load(f)\n",
    "with open('train_stackfeats_5_6.pickle', 'rb') as f:\n",
    "    train3 = pickle.load(f)\n",
    "with open('test_stackfeats_7_8.pickle', 'rb') as f:\n",
    "    test_list4 = pickle.load(f)\n",
    "with open('train_stackfeats_7_8.pickle', 'rb') as f:\n",
    "    train4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = test_list1\n",
    "for i in range(5):\n",
    "    test_list[i] = test_list[i].merge(test_list2[i], on='ID', how='left')\n",
    "    test_list[i] = test_list[i].merge(test_list3[i], on='ID', how='left')\n",
    "    test_list[i] = test_list[i].merge(test_list4[i], on='ID', how='left')\n",
    "train = train1\n",
    "train = train.merge(train2.drop(target_cols + ['StudyInstanceUID', 'folds'], axis=1), on='ID', how='left')\n",
    "train = train.merge(train3.drop(target_cols + ['StudyInstanceUID', 'folds'], axis=1), on='ID', how='left')\n",
    "train = train.merge(train4.drop(target_cols + ['StudyInstanceUID', 'folds'], axis=1), on='ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_8models_stackfeats.pickle', 'wb') as f:\n",
    "    pickle.dump(test_list, f)\n",
    "with open('train_8model_stackfeats.pickle', 'wb') as f:\n",
    "    pickle.dump(train, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_8models_stackfeats.pickle', 'rb') as f:\n",
    "    test_list = pickle.load(f)\n",
    "with open('train_8model_stackfeats.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = test_list[0].columns.drop(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "stack_preds = []\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.1,\n",
    "          \"num_leaves\": 5,\n",
    "           \"max_bin\": 256,\n",
    "          \"feature_fraction\": 0.8,\n",
    "          \"verbosity\": 0,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_child_weight\": 150,\n",
    "          \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.9\n",
    "          }\n",
    "res = []\n",
    "pred_test = []\n",
    "fi = {}\n",
    "for c in target_cols:\n",
    "    fi[c] = []\n",
    "stack_preds_valid = []\n",
    "for i in range(5):\n",
    "    tr = train.query('folds != @i')\n",
    "    va = train.query('folds == @i')\n",
    "    preds = pd.DataFrame(np.zeros([len(va), 6]), columns = [c + '_pred' for c in target_cols])\n",
    "    for tar_col in target_cols:\n",
    "        tr_D = lgb.Dataset(tr[X_cols], tr[tar_col])\n",
    "        va_D = lgb.Dataset(va[X_cols], va[tar_col])\n",
    "        clf = lgb.train(params, tr_D, 10000, valid_sets=va_D, verbose_eval=100,\n",
    "                                    early_stopping_rounds=120)\n",
    "        preds[tar_col + '_pred'] = clf.predict(va[X_cols])\n",
    "        pred_test.append(clf.predict(test_list[i][X_cols]))\n",
    "        df_fi = pd.DataFrame(clf.feature_importance(importance_type='gain'), index = X_cols, columns=['FI_score'])\n",
    "        fi[tar_col].append(df_fi)\n",
    "        print(log_loss(va[tar_col], preds[tar_col + '_pred']))\n",
    "        res.append(log_loss(va[tar_col], preds[tar_col + '_pred']))\n",
    "        print('-'*80)\n",
    "    print('='*80)\n",
    "    stack_preds.append([va['ID'], preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(res):\n",
    "    score = 0\n",
    "    for i, r in enumerate(res):\n",
    "        if i %6 == 0:\n",
    "            score += 2*r\n",
    "        else:\n",
    "            score += r\n",
    "    return score/5/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../prediction/appian/'\n",
    "n_fold = 0\n",
    "test = test_list[0]\n",
    "sub = pd.read_pickle(f'{path}/fold{n_fold}_ep2_test_tta5.pkl')\n",
    "preds_test_df = pd.DataFrame()\n",
    "for n_fold in range(5):\n",
    "    for n_target in range(6):\n",
    "        preds_test_df[target_cols[n_target] + '_' + str(n_fold) + 'fold'] = pred_test[n_fold*6 + n_target]\n",
    "preds_test_df.index = test['ID'].values\n",
    "preds_test_df = preds_test_df.loc[sub[0]['ids']]\n",
    "pred_test_sorted_list = []\n",
    "for i in range(30):\n",
    "    pred_test_sorted_list.append(preds_test_df.iloc[:, i].values)\n",
    "pred_test_df_list = []\n",
    "for n_fold in range(5):\n",
    "    tmp = np.zeros([len(test), 6])\n",
    "    for n_tar in range(6):\n",
    "        tmp[:, n_tar] = pred_test_sorted_list[6*n_fold + n_tar]\n",
    "    pred_test_df = pd.DataFrame(tmp, columns = target_cols)\n",
    "    pred_test_df_list.append(pred_test_df)\n",
    "sub = pd.read_pickle(f'{path}/fold{n_fold}_ep2_test_tta5.pkl')\n",
    "sub_original = pd.read_pickle(f'{path}/fold{n_fold}_ep2_test_tta5.pkl')\n",
    "for i in range(5):\n",
    "    sub[i]['outputs'] = pred_test_df_list[i][target_cols].values\n",
    "for i in range(5):\n",
    "    for ii in range(6):\n",
    "        print(np.corrcoef(sub[i]['outputs'][:, ii], sub_original[i]['outputs'][:, ii])[0, 1])\n",
    "path_to = '../stack_prediction/pred_test_1103_4.pkl'\n",
    "with open(path_to, 'wb') as f:\n",
    "    pickle.dump(sub, f)\n",
    "path_to = f'../stack_prediction/pred_valid_1103_4.pkl'\n",
    "with open(path_to, 'wb') as f:\n",
    "    pickle.dump(stack_preds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the correlation of submits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_csv('../model/model001/stack_1031_nobag.csv')\n",
    "a2 = pd.read_csv('../model/seresnext410/stack_1031_resnext410.csv')\n",
    "np.corrcoef(a1['Label'], a2['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_csv('../model/seresnext410/stack_1031_resnext410_dart.csv')\n",
    "a2 = pd.read_csv('../model/seresnext410/stack_1031_resnext410.csv')\n",
    "np.corrcoef(a1['Label'], a2['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_csv('../model/model001/stack_1031_nobag.csv')\n",
    "a2 = pd.read_csv('../model/inceptionv4/stack_1031_inceptionv4.csv')\n",
    "np.corrcoef(a1['Label'], a2['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv('../model/inceptionv4/stack_1031_inceptionv4.csv')\n",
    "p2 = pd.read_csv('../model/seresnext410/stack_1031_resnext410.csv')\n",
    "p3 = pd.read_csv('../model/seresnext410/stack_1031_resnext410_dart.csv')\n",
    "p4 = pd.read_csv('../model/model001/stack_1031_2.csv')\n",
    "p5 = pd.read_csv('../model/model001/stack_1031_model001_dart.csv')\n",
    "p6 = pd.read_csv('../model/inceptionv4/stack_1031_inceptionv4_dart.csv')\n",
    "p = p1['Label'] * 0.15 + p5['Label'] * 0.15 + p2['Label'] * 0.15 + p3['Label'] * 0.15 + p4['Label'] * 0.2 + p6['Label'] * 0.2\n",
    "a1['Label'] = p.values * 0.98\n",
    "a1.to_csv('1031_stack_seresnext512_30_seresnext410_25_seresnext410dart_25_seresnext410_20_2_multi098.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_csv('../model/model001/stack_1031_nobag.csv')\n",
    "a2 = pd.read_csv('../model/model001/stack_1031_2.csv')\n",
    "np.corrcoef(a1['Label'], a2['Label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
